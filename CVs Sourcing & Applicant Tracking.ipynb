{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T10:32:47.427034Z",
     "iopub.status.busy": "2025-08-24T10:32:47.426182Z",
     "iopub.status.idle": "2025-08-24T10:32:50.669902Z",
     "shell.execute_reply": "2025-08-24T10:32:50.669096Z",
     "shell.execute_reply.started": "2025-08-24T10:32:47.426999Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T10:32:50.672332Z",
     "iopub.status.busy": "2025-08-24T10:32:50.671990Z",
     "iopub.status.idle": "2025-08-24T10:32:53.875390Z",
     "shell.execute_reply": "2025-08-24T10:32:53.874448Z",
     "shell.execute_reply.started": "2025-08-24T10:32:50.672294Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
      "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install PyPDF2 python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-24T18:04:54.721Z",
     "iopub.execute_input": "2025-08-24T10:32:58.396298Z",
     "iopub.status.busy": "2025-08-24T10:32:58.395998Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 10:33:04.322037: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756031584.345527      94 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756031584.352703      94 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "* Running on public URL: https://54e9ecfa6622e51244.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://54e9ecfa6622e51244.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen 2.5 1.5B model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b809e37fc34f4780a0855896ee5353d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddf8e2e6ad443c099142a4270e901ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4a6996979b4e77b508a066564832ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687007aa6f3547d086472c885b52bb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed8ea3604974d0084300d4482c1ea50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4032d0c070bb41e587cd1806e60feb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac2a7f1212f48d79caea992b86ce1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sentence Transformer models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961a276579fd426fa63ba883f0af28cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a979a95308d416a8875eda604dd56a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a851ce2c314a489183e4ffeefe3bd80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01360116c6ad46648dd241cd8877c39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bad92dc81044727a6f3aed843cf05d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572524a6cefa4823896b80fbe2b93602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439536f61ed34a7490781c1c77b3f28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05374dad6f84fe29fa30c700e32a4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bd869b05844994aac4e3913b6c7577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df39d4ab25e48729bd701beabd84890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed3639a21f24fb69c66db0e43531cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# CV/LinkedIn Matcher + Clean CSV + Gradio GUI (single cell)\n",
    "# =======================\n",
    "\n",
    "import os, re, json, tempfile, logging\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import PyPDF2, docx\n",
    "\n",
    "# ---- optional OCR (PNG/JPG/WEBP) ----\n",
    "OCR_AVAILABLE = False\n",
    "try:\n",
    "    from PIL import Image\n",
    "    import pytesseract\n",
    "    tpath = os.environ.get(\"TESSERACT_PATH\")\n",
    "    if tpath:\n",
    "        pytesseract.pytesseract.tesseract_cmd = tpath  # e.g. \"C:/Program Files/Tesseract-OCR/tesseract.exe\"\n",
    "    OCR_AVAILABLE = True\n",
    "except Exception:\n",
    "    OCR_AVAILABLE = False\n",
    "\n",
    "# Reduce library chattiness\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"sentence_transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# ========= Base system =========\n",
    "class CVMatchingSystem:\n",
    "    def __init__(self):\n",
    "        print(\"Loading Qwen 2.5 1.5B model...\")\n",
    "        self.model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "        dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(\"Loading Sentence Transformer models...\")\n",
    "        self.responsibilities_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.degree_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"All models loaded successfully!\")\n",
    "\n",
    "    # ---------- OCR ----------\n",
    "    def extract_text_from_image(self, file_path: str) -> str:\n",
    "        if not OCR_AVAILABLE:\n",
    "            return \"\"\n",
    "        try:\n",
    "            img = Image.open(file_path)\n",
    "            txt = pytesseract.image_to_string(img)\n",
    "            return txt or \"\"\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    def extract_text_from_file(self, file_path):\n",
    "        \"\"\"PDF/DOCX/TXT/RTF/PNG/JPG/JPEG/WEBP\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            lower = file_path.lower()\n",
    "            if lower.endswith('.pdf'):\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    for page in reader.pages:\n",
    "                        text += (page.extract_text() or \"\") + \"\\n\"\n",
    "            elif lower.endswith('.docx'):\n",
    "                doc_ = docx.Document(file_path)\n",
    "                for p in doc_.paragraphs:\n",
    "                    text += p.text + \"\\n\"\n",
    "            elif lower.endswith(('.txt', '.rtf')):\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    text = f.read()\n",
    "            elif lower.endswith(('.png', '.jpg', '.jpeg', '.webp')):\n",
    "                text = self.extract_text_from_image(file_path)\n",
    "            else:\n",
    "                return None\n",
    "        except Exception:\n",
    "            return None\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def _norm_spaces(s: str) -> str:\n",
    "        return re.sub(r'\\s+', ' ', s or '').strip()\n",
    "\n",
    "    def split_text(self, text):\n",
    "        text = self._norm_spaces(text)\n",
    "        if not text:\n",
    "            return \"\", \"\"\n",
    "        mid = len(text)//2\n",
    "        pos = mid\n",
    "        for i in range(mid, min(mid+500, len(text))):\n",
    "            if text[i] in \".!?\" and (i+1 >= len(text) or text[i+1] == \" \"):\n",
    "                pos = i+1; break\n",
    "        if pos == mid:\n",
    "            for i in range(mid, max(mid-500, 0), -1):\n",
    "                if text[i] in \".!?\" and (i+1 >= len(text) or text[i+1] == \" \"):\n",
    "                    pos = i+1; break\n",
    "        return text[:pos].strip(), text[pos:].strip()\n",
    "\n",
    "    def get_first_10_lines(self, text):\n",
    "        lines = re.split(r'\\r|\\n', text or \"\")\n",
    "        if len(lines) == 1:\n",
    "            chunk = 120\n",
    "            lines = [text[i:i+chunk] for i in range(0, len(text), chunk)]\n",
    "        return \"\\n\".join(lines[:10])\n",
    "\n",
    "    def extract_years_from_pattern(self, text):\n",
    "        pats = [\n",
    "            r'(\\d+)\\s*\\+\\s*years?\\s+of\\s+experience',\n",
    "            r'\\+\\s*(\\d+)\\s+years?\\s+of\\s+experience',\n",
    "            r'(\\d+)\\s+years?\\s+of\\s+experience',\n",
    "            r'(\\d+)\\s*\\+\\s*year\\s+of\\s+experience',\n",
    "            r'\\+\\s*(\\d+)\\s+year\\s+of\\s+experience',\n",
    "            r'(\\d+)\\s+year\\s+of\\s+experience'\n",
    "        ]\n",
    "        maxy = 0\n",
    "        for p in pats:\n",
    "            m = re.findall(p, text or \"\", re.I)\n",
    "            if m:\n",
    "                ys = [int(x) for x in m]\n",
    "                maxy = max(maxy, max(ys))\n",
    "        return maxy\n",
    "\n",
    "    def query_llm(self, prompt):\n",
    "        try:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts specific information from CVs/resumes. Be concise.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
    "            out = self.model.generate(\n",
    "                **inputs, max_new_tokens=256, do_sample=True, temperature=0.1,\n",
    "                top_p=0.9, pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            out = [o[len(i):] for i, o in zip(inputs.input_ids, out)]\n",
    "            return self.tokenizer.batch_decode(out, skip_special_tokens=True)[0].strip()\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    def query_llm_batch(self, prompts, max_new_tokens=128):\n",
    "        try:\n",
    "            sys = {\"role\": \"system\", \"content\": \"You extract info from CVs/LinkedIn. Be concise.\"}\n",
    "            texts = []\n",
    "            for p in prompts:\n",
    "                msgs = [sys, {\"role\": \"user\", \"content\": p}]\n",
    "                texts.append(self.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True))\n",
    "            inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True).to(self.model.device)\n",
    "            out = self.model.generate(\n",
    "                **inputs, max_new_tokens=max_new_tokens, do_sample=False,\n",
    "                temperature=0.0, top_p=1.0, pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            res = []\n",
    "            for i in range(len(texts)):\n",
    "                gen_only = out[i, inputs.input_ids[i].shape[0]:]\n",
    "                res.append(self.tokenizer.decode(gen_only, skip_special_tokens=True).strip())\n",
    "            return res\n",
    "        except Exception:\n",
    "            return [\"\" for _ in prompts]\n",
    "\n",
    "    def create_feature_prompt(self, cv_part, feature_name, context=\"\"):\n",
    "        base = f\"Extract information from this CV text: {context}\\n\\n{cv_part}\\n\\n\"\n",
    "        prompts = {\n",
    "            \"Responsibilities\": base + \"Extract key job responsibilities and work duties. Return concise description:\",\n",
    "            \"Degree\":          base + \"Extract highest educational degree (Bachelor's/Master's/PhD). Format: 'Degree in Major':\",\n",
    "            \"Skills\":          base + \"Extract technical and professional skills as comma-separated values:\",\n",
    "            \"GPA\":             base + \"Extract GPA number only:\",\n",
    "            \"Achievements\":    base + \"Extract achievements and awards as comma-separated values:\",\n",
    "            \"Volunteering\":    base + \"Extract volunteering experiences as comma-separated values:\",\n",
    "            \"Candidate Name\":  base + \"Extract the person's FULL NAME only (no extra words):\",\n",
    "            \"Current Title\":   base + \"Extract the most recent or primary job title (short):\",\n",
    "            \"LinkedIn URL\":    base + \"Extract the LinkedIn profile URL, if present. Return only the URL or empty:\",\n",
    "            \"Contact\":         base + \"Extract email and phone in one line 'email | phone' (leave blank if missing):\",\n",
    "        }\n",
    "        return prompts.get(feature_name, base + f\"{feature_name}:\")\n",
    "\n",
    "    def extract_feature_from_part(self, cv_part, feature_name, context=\"\"):\n",
    "        return self.query_llm(self.create_feature_prompt(cv_part, feature_name, context))\n",
    "\n",
    "    def combine_feature_results(self, a, b, feature_name):\n",
    "        if feature_name in [\"Skills\", \"Achievements\", \"Volunteering\"]:\n",
    "            combined = []\n",
    "            for r in [a, b]:\n",
    "                if r and r.strip():\n",
    "                    items = [i.strip() for i in r.split(\",\") if i.strip()]\n",
    "                    combined.extend(items)\n",
    "            seen = set()\n",
    "            out = []\n",
    "            for i in combined:\n",
    "                k = i.lower()\n",
    "                if k in seen: \n",
    "                    continue\n",
    "                seen.add(k); out.append(i)\n",
    "            return out\n",
    "        elif feature_name == \"GPA\":\n",
    "            g1 = self.extract_gpa(a) if a else 0.0\n",
    "            g2 = self.extract_gpa(b) if b else 0.0\n",
    "            return f\"{max(g1, g2):.2f}\" if max(g1, g2) > 0 else \"2.00\"\n",
    "        else:\n",
    "            a, b = (a or \"\").strip(), (b or \"\").strip()\n",
    "            if not a and not b: return \"\"\n",
    "            if not a: return b\n",
    "            if not b: return a\n",
    "            return a if len(a) >= len(b) else b\n",
    "\n",
    "    def extract_gpa(self, text):\n",
    "        pats = [r'GPA\\s*[:]?\\s*(\\d\\.\\d+)', r'(\\d\\.\\d+)\\s*GPA', r'(\\d\\.\\d+)\\s*/\\s*4\\.0']\n",
    "        for p in pats:\n",
    "            m = re.findall(p, text or \"\", re.I)\n",
    "            if m:\n",
    "                return max(float(x) for x in m)\n",
    "        dec = re.findall(r'\\b(\\d\\.\\d+)\\b', text or \"\")\n",
    "        if dec:\n",
    "            cand = [float(x) for x in dec if 1.0 <= float(x) <= 4.0]\n",
    "            return max(cand) if cand else 0.0\n",
    "        return 0.0\n",
    "\n",
    "    def parse_contacts_and_links(self, text: str) -> Dict[str, str]:\n",
    "        linkedin = \"\"\n",
    "        m = re.search(r'(https?://(?:www\\.)?linkedin\\.com/[A-Za-z0-9/_\\-\\?\\=&%\\.]+)', text or \"\", re.I)\n",
    "        if m: linkedin = m.group(1).strip().rstrip(').,;')\n",
    "        emails = re.findall(r'[A-Za-z0-9._%+\\-]+@[A-Za-z0-9.\\-]+\\.[A-Za-z]{2,}', text or \"\")\n",
    "        phones = re.findall(r'(\\+?\\d[\\d\\s\\-\\(\\)]{7,}\\d)', text or \"\")\n",
    "        email = emails[0] if emails else \"\"\n",
    "        phone = re.sub(r'[^\\d+]', '', phones[0]) if phones else \"\"\n",
    "        contact = \" | \".join([p for p in [email, phone] if p])\n",
    "        return {\"LinkedIn URL\": linkedin, \"Email\": email, \"Phone\": phone, \"Contact\": contact}\n",
    "\n",
    "    def extract_job_requirements(self, job_description: str):\n",
    "        \"\"\"LLM extraction from JD. If JD empty -> neutral requirements.\"\"\"\n",
    "        if not (job_description or \"\").strip():\n",
    "            return {\n",
    "                \"Responsibilities\": \"\",\n",
    "                \"Degree\": \"\",\n",
    "                \"Years of Experience\": 0,\n",
    "                \"Skills\": [],\n",
    "                \"Plus Skills\": []\n",
    "            }\n",
    "        prompts = {\n",
    "            \"Responsibilities\": f\"Extract key responsibilities from job description:\\n{job_description}\\nResponsibilities:\",\n",
    "            \"Degree\": f\"Extract required degree from job description:\\n{job_description}\\nRequired Degree:\",\n",
    "            \"Years of Experience\": f\"Extract required years of experience from job description:\\n{job_description}\\nYears Required:\",\n",
    "            \"Skills\": f\"Extract required skills from job description as comma-separated values:\\n{job_description}\\nRequired Skills:\",\n",
    "            \"Plus Skills\": f\"Extract preferred/plus skills from job description as comma-separated values:\\n{job_description}\\nPlus Skills:\"\n",
    "        }\n",
    "        res = {k: self.query_llm(p) for k, p in prompts.items()}\n",
    "        years = 0\n",
    "        years_text = res.get(\"Years of Experience\", \"\")\n",
    "        for pat in [r'(\\d+)\\s*\\+\\s*years?', r'(\\d+)\\s+years?', r'(\\d+)\\s*[-]+\\s*years?']:\n",
    "            m = re.findall(pat, years_text or \"\", re.I)\n",
    "            if m:\n",
    "                years = max(int(x) for x in m); break\n",
    "        res[\"Years of Experience\"] = years\n",
    "        for s in [\"Skills\", \"Plus Skills\"]:\n",
    "            if res.get(s):\n",
    "                res[s] = list({i.strip() for i in res[s].split(\",\") if i.strip()})\n",
    "            else:\n",
    "                res[s] = []\n",
    "        return res\n",
    "\n",
    "    def calculate_matching_scores(self, candidate_features, job_requirements):\n",
    "        scores = {}\n",
    "        # 1. Responsibilities (19%)\n",
    "        rc = candidate_features.get(\"Responsibilities\", \"\")\n",
    "        rj = job_requirements.get(\"Responsibilities\", \"\")\n",
    "        if rc and rj:\n",
    "            e1 = self.responsibilities_model.encode(rc, convert_to_tensor=True)\n",
    "            e2 = self.responsibilities_model.encode(rj, convert_to_tensor=True)\n",
    "            scores[\"responsibilities\"] = util.pytorch_cos_sim(e1, e2).item()\n",
    "        else:\n",
    "            scores[\"responsibilities\"] = 0.0\n",
    "        # 2. Degree (19%)\n",
    "        dc = candidate_features.get(\"Degree\", \"\")\n",
    "        dj = job_requirements.get(\"Degree\", \"\")\n",
    "        if dc and dj:\n",
    "            e1 = self.degree_model.encode(dc, convert_to_tensor=True)\n",
    "            e2 = self.degree_model.encode(dj, convert_to_tensor=True)\n",
    "            scores[\"degree\"] = util.pytorch_cos_sim(e1, e2).item()\n",
    "        else:\n",
    "            scores[\"degree\"] = 0.0\n",
    "        # 3. Experience (19%)\n",
    "        ec = candidate_features.get(\"Years of Experience\", 0)\n",
    "        ej = job_requirements.get(\"Years of Experience\", 0)\n",
    "        scores[\"experience\"] = 1.0 if ec >= ej else ec / max(ej, 1)\n",
    "        # 4. Skills (19%)\n",
    "        sc = set(candidate_features.get(\"Skills\", []))\n",
    "        sj = set(job_requirements.get(\"Skills\", []))\n",
    "        scores[\"skills\"] = (len(sc & sj) / len(sj)) if sj else 0.0\n",
    "        # 5. Plus skills (5%)\n",
    "        pc = set(candidate_features.get(\"Plus Skills\", []))\n",
    "        pj = set(job_requirements.get(\"Plus Skills\", []))\n",
    "        scores[\"plus_skills\"] = (len(pc & pj) / len(pj)) if pj else 0.0\n",
    "        # 6. Extra skills (5%)\n",
    "        allc = set(candidate_features.get(\"Skills\", []) + candidate_features.get(\"Plus Skills\", []))\n",
    "        req = sj | pj\n",
    "        extra = allc - req\n",
    "        scores[\"extra_skills\"] = min(len(extra)/10, 1.0)\n",
    "        # 7. GPA (5%)\n",
    "        gpa = float(candidate_features.get(\"GPA\", \"2.00\") or 2.0)\n",
    "        scores[\"gpa\"] = min(gpa/4.0, 1.0)\n",
    "        # 8. Achievements (6%)\n",
    "        ach = len(candidate_features.get(\"Achievements\", []))\n",
    "        scores[\"achievements\"] = min(ach/5.0, 1.0)\n",
    "        # 9. Volunteering (3%)\n",
    "        vol = len(candidate_features.get(\"Volunteering\", []))\n",
    "        scores[\"volunteering\"] = min(vol/2.0, 1.0)\n",
    "        return scores\n",
    "\n",
    "    def calculate_final_score(self, scores):\n",
    "        weights = {\n",
    "            'responsibilities': 0.19, 'degree': 0.19, 'experience': 0.19, 'skills': 0.19,\n",
    "            'plus_skills': 0.05, 'extra_skills': 0.05, 'gpa': 0.05, 'achievements': 0.06, 'volunteering': 0.03\n",
    "        }\n",
    "        total = 0.0\n",
    "        for k, w in weights.items():\n",
    "            total += scores.get(k, 0.0) * w\n",
    "        return min(total, 1.0)\n",
    "\n",
    "    def process_cv(self, file_path):\n",
    "        txt = self.extract_text_from_file(file_path)\n",
    "        if not txt:\n",
    "            return None\n",
    "        guess = self.parse_contacts_and_links(txt)\n",
    "        p1, p2 = self.split_text(txt)\n",
    "        first10 = self.get_first_10_lines(p1)\n",
    "        years = self.extract_years_from_pattern(first10)\n",
    "        feats = {\"Years of Experience\": years if years > 0 else 0}\n",
    "        # core features\n",
    "        core = [\"Responsibilities\", \"Degree\", \"Skills\", \"GPA\", \"Achievements\", \"Volunteering\"]\n",
    "        for f in core:\n",
    "            r1 = self.extract_feature_from_part(p1, f)\n",
    "            ctx = f\"First part found: {r1[:100]}\" if r1 else \"\"\n",
    "            r2 = self.extract_feature_from_part(p2, f, ctx)\n",
    "            feats[f] = self.combine_feature_results(r1, r2, f)\n",
    "        feats[\"Plus Skills\"] = feats.get(\"Skills\", [])[:]\n",
    "        # identity + contact\n",
    "        id_feats = [\"Candidate Name\", \"Current Title\", \"LinkedIn URL\", \"Contact\"]\n",
    "        prompts1 = [self.create_feature_prompt(p1, f) for f in id_feats]\n",
    "        res1 = self.query_llm_batch(prompts1, max_new_tokens=96)\n",
    "        prompts2 = [\n",
    "            self.create_feature_prompt(p2, feat, context=f\"First part found: {a[:80]}\")\n",
    "            for feat, a in zip(id_feats, res1)\n",
    "        ]\n",
    "        res2 = self.query_llm_batch(prompts2, max_new_tokens=96)\n",
    "        for feat, a, b in zip(id_feats, res1, res2):\n",
    "            feats[feat] = self.combine_feature_results(a, b, feat)\n",
    "        # fill from regex if missing\n",
    "        feats.setdefault(\"LinkedIn URL\", guess.get(\"LinkedIn URL\", \"\"))\n",
    "        if not feats.get(\"LinkedIn URL\"): feats[\"LinkedIn URL\"] = guess.get(\"LinkedIn URL\", \"\")\n",
    "        if not feats.get(\"Contact\"):       feats[\"Contact\"] = guess.get(\"Contact\", \"\")\n",
    "        return feats\n",
    "\n",
    "    def process_candidates(self, folder_path, job_description):\n",
    "        if not os.path.isdir(folder_path):\n",
    "            raise FileNotFoundError(f\"Folder not found: {folder_path}\")\n",
    "        job_req = self.extract_job_requirements(job_description)\n",
    "        exts = ['.pdf', '.docx', '.txt', '.rtf', '.png', '.jpg', '.jpeg', '.webp']\n",
    "        files = [os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "                 if any(f.lower().endswith(x) for x in exts)]\n",
    "        if not files:\n",
    "            return []\n",
    "        results = []\n",
    "        for f in files:\n",
    "            feats = self.process_cv(f)\n",
    "            if not feats: \n",
    "                continue\n",
    "            scores = self.calculate_matching_scores(feats, job_req)\n",
    "            results.append({\n",
    "                \"filename\": os.path.basename(f),\n",
    "                \"features\": feats,\n",
    "                \"scores\": scores,\n",
    "                \"final_score\": self.calculate_final_score(scores)\n",
    "            })\n",
    "        results.sort(key=lambda x: x['final_score'], reverse=True)\n",
    "        return results\n",
    "\n",
    "# ========= Clean CSV helpers =========\n",
    "import pandas as pd\n",
    "\n",
    "def _strip_ws(s: str) -> str:\n",
    "    s = (s or \"\")\n",
    "    s = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]\", \" \", s)  # drop control chars\n",
    "    s = s.replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _clean_cell(x, replace_commas=True, max_len=180) -> str:\n",
    "    if isinstance(x, (list, tuple, set)):\n",
    "        x = \" | \".join([_strip_ws(str(i)) for i in x if str(i).strip()])\n",
    "    else:\n",
    "        x = _strip_ws(str(x))\n",
    "    if replace_commas:\n",
    "        x = x.replace(\",\", \" Â·\")\n",
    "    if max_len and len(x) > max_len:\n",
    "        x = x[: max_len - 1] + \"â€¦\"\n",
    "    return x\n",
    "\n",
    "def _norm_skills(skills, topn=10):\n",
    "    out, seen = [], set()\n",
    "    for s in skills or []:\n",
    "        s = _strip_ws(str(s))\n",
    "        s = re.sub(r\"^[\\-\\â€¢\\Â·\\|]+\", \"\", s).strip(\" ,;:|-\")\n",
    "        if not s: \n",
    "            continue\n",
    "        k = s.lower()\n",
    "        if k in seen: \n",
    "            continue\n",
    "        seen.add(k); out.append(s)\n",
    "        if len(out) >= topn:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def _split_contact(contact: str):\n",
    "    contact = _strip_ws(contact)\n",
    "    email_match = re.search(r\"[A-Za-z0-9._%+\\-]+@[A-Za-z0-9.\\-]+\\.[A-Za-z]{2,}\", contact)\n",
    "    phone_match = re.search(r\"(\\+?\\d[\\d\\s\\-\\(\\)]{7,}\\d)\", contact)\n",
    "    email = email_match.group(0) if email_match else \"\"\n",
    "    phone = re.sub(r\"[^\\d+]\", \"\", phone_match.group(0)) if phone_match else \"\"\n",
    "    return email, phone\n",
    "\n",
    "def build_summary_df(results, topk: int = 10) -> pd.DataFrame:\n",
    "    top = results[:topk]\n",
    "    rows = []\n",
    "    for i, r in enumerate(top, 1):\n",
    "        feats = r.get(\"features\", {}) or {}\n",
    "        skills = _norm_skills(feats.get(\"Skills\", []), topn=10)\n",
    "        email, phone = _split_contact(feats.get(\"Contact\", \"\"))\n",
    "\n",
    "        row = {\n",
    "            \"rank\": i,\n",
    "            \"filename\": _clean_cell(r.get(\"filename\", \"\"), True, 120),\n",
    "            \"name\": _clean_cell(feats.get(\"Candidate Name\", \"\"), True, 80),\n",
    "            \"title\": _clean_cell(feats.get(\"Current Title\", \"\"), True, 100),\n",
    "            \"linkedin\": _clean_cell(feats.get(\"LinkedIn URL\", \"\"), False, 140),\n",
    "            \"email\": _clean_cell(email, True, 120),\n",
    "            \"phone\": _clean_cell(phone, True, 30),\n",
    "            \"final_score(%)\": round(100 * float(r.get(\"final_score\", 0.0)), 2),\n",
    "            \"years_exp\": int(feats.get(\"Years of Experience\", 0) or 0),\n",
    "            \"degree\": _clean_cell(feats.get(\"Degree\", \"\"), True, 120),\n",
    "            \"gpa\": _clean_cell(feats.get(\"GPA\", \"\"), True, 8),\n",
    "            \"skills_top10\": _clean_cell(\" | \".join(skills), True, 300),\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "    cols = [\"rank\",\"filename\",\"linkedin\",\"email\",\"phone\", \"final_score(%)\"]\n",
    "    return pd.DataFrame(rows, columns=cols).fillna(\"\")\n",
    "\n",
    "def save_outputs(summary_df: pd.DataFrame, results):\n",
    "    out_dir = tempfile.mkdtemp(prefix=\"cv_match_out_\")\n",
    "    csv_path = os.path.join(out_dir, \"cv_matching_summary.csv\")\n",
    "    json_path = os.path.join(out_dir, \"cv_matching_results.json\")\n",
    "    summary_df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results[:10], f, ensure_ascii=False, indent=2)\n",
    "    return csv_path, json_path\n",
    "\n",
    "# ========= Gradio UI =========\n",
    "import gradio as gr\n",
    "\n",
    "# cache the heavy system\n",
    "_SYSTEM = None\n",
    "def get_system():\n",
    "    global _SYSTEM\n",
    "    if _SYSTEM is None:\n",
    "        _SYSTEM = CVMatchingSystem()\n",
    "    return _SYSTEM\n",
    "\n",
    "DEFAULT_JD = \"\"  # JD optional; leave blank if you don't want to use one\n",
    "\n",
    "def run_match(files: list, jd_text: str, topk: int):\n",
    "    if not files:\n",
    "        return \"Please upload at least one file.\", None, None, None\n",
    "\n",
    "    # write uploads to a temp folder\n",
    "    work_dir = tempfile.mkdtemp(prefix=\"cvs_\")\n",
    "    for f in files:\n",
    "        # f is a tempfile.NamedTemporaryFile or path-like (gradio)\n",
    "        src = f.name if hasattr(f, \"name\") else str(f)\n",
    "        dst = os.path.join(work_dir, os.path.basename(src))\n",
    "        with open(src, \"rb\") as r, open(dst, \"wb\") as w:\n",
    "            w.write(r.read())\n",
    "\n",
    "    # run matcher\n",
    "    sys = get_system()\n",
    "    results = sys.process_candidates(work_dir, jd_text or \"\")\n",
    "    if not results:\n",
    "        return \"No readable candidate files were found.\", None, None, None\n",
    "\n",
    "    df = build_summary_df(results, topk=topk)\n",
    "    csv_path, json_path = save_outputs(df, results)\n",
    "    msg = f\"Processed {len(results)} files. Showing top {min(topk, len(results))}.\"\n",
    "    return msg, df, csv_path, json_path\n",
    "\n",
    "with gr.Blocks(title=\"CV/LinkedIn Matcher\") as demo:\n",
    "    gr.Markdown(\"## ðŸ”Ž CV / LinkedIn Matcher\\nUpload CVs and/or LinkedIn screenshots. JD is optional.\")\n",
    "    with gr.Row():\n",
    "        files = gr.File(label=\"Upload files (.pdf, .docx, .txt, .rtf, .png, .jpg, .jpeg, .webp)\", file_count=\"multiple\")\n",
    "        jd = gr.Textbox(label=\"Job Description (optional)\", value=DEFAULT_JD, lines=6, placeholder=\"Leave blank to skip JD-based matching.\")\n",
    "        topk = gr.Slider(1, 50, value=10, step=1, label=\"Top N to show / export\")\n",
    "    run_btn = gr.Button(\"Run matching\", variant=\"primary\")\n",
    "    status = gr.Markdown()\n",
    "    table = gr.Dataframe(interactive=False, wrap=True, label=\"Top Matches Preview\")\n",
    "    csv_file = gr.File(label=\"Download CSV\")\n",
    "    json_file = gr.File(label=\"Download JSON (top N)\")\n",
    "\n",
    "    run_btn.click(fn=run_match, inputs=[files, jd, topk], outputs=[status, table, csv_file, json_file])\n",
    "\n",
    "demo.launch(debug=True)  # in Kaggle this shows inline; no need for share=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8126351,
     "sourceId": 12848234,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
